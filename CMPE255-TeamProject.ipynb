{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data from unzipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention: You need to unzip \"reviews_Home_and_Kitchen_5.json\" to get \"Home_and_Kitchen_5.json\"\n",
    "# I used \"WinRar\" on Windows to extract this json file\n",
    "path = \"Home_and_Kitchen_5.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path, encoding='utf-8') as f:\n",
    "    data = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "551682"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proccessed_data = []\n",
    "proccessed_data = [ [dics[\"reviewText\"], dics[\"helpful\"], dics['overall'] ] for dics in data]\n",
    "# for dics in data:\n",
    "#     line = []\n",
    "#     line.append(dics[\"reviewText\"])\n",
    "#     line.append(dics[\"helpful\"])\n",
    "#     line.append(dics['overall'])\n",
    "#     proccessed_data.append(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking number of unique words in the data set before preproccessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist1 = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 2772778, 'i': 1591274, 'and': 1541022, 'a': 1434474, 'to': 1416472, 'it': 1226405, 'is': 863426, 'of': 832332, 'this': 732618, 'for': 715249, ...})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for line in proccessed_data:\n",
    "    for word in line[0].split():\n",
    "        fdist1[word.lower()] += 1\n",
    "fdist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 838417 samples and 53925892 outcomes>\n"
     ]
    }
   ],
   "source": [
    "print(fdist1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np_data = np.array(proccessed_data)\n",
    "# print(np_data) \n",
    "# len(np_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Filtering Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_set = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_punctuation(text):\n",
    "    no_punct=[words for words in text if words not in punct_set]\n",
    "    words_wo_punct=''.join(no_punct)\n",
    "    return words_wo_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in proccessed_data:\n",
    "    data[0] = filter_punctuation(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 481174 samples and 53739451 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fdist_wo_punct = FreqDist()\n",
    "for line in proccessed_data:\n",
    "    for word in line[0].split():\n",
    "        fdist_wo_punct[word.lower()] += 1\n",
    "print(fdist_wo_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of unique words went down to 481174 from 838417 after removing the puncuation :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gaston\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in proccessed_data:\n",
    "    data[0] = word_tokenize(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 2780773),\n",
       " ('i', 1606253),\n",
       " ('and', 1557137),\n",
       " ('a', 1438350),\n",
       " ('to', 1425398),\n",
       " ('it', 1387669),\n",
       " ('is', 876047),\n",
       " ('of', 835434),\n",
       " ('this', 759706),\n",
       " ('for', 728397),\n",
       " ('in', 612086),\n",
       " ('that', 551863),\n",
       " ('my', 467378),\n",
       " ('with', 446288),\n",
       " ('you', 432856),\n",
       " ('have', 428896),\n",
       " ('on', 425902),\n",
       " ('but', 399901),\n",
       " ('not', 355774),\n",
       " ('so', 322844),\n",
       " ('as', 308031),\n",
       " ('was', 305702),\n",
       " ('are', 304115),\n",
       " ('one', 270744),\n",
       " ('use', 243819),\n",
       " ('they', 241978),\n",
       " ('be', 239917),\n",
       " ('its', 239749),\n",
       " ('very', 232904),\n",
       " ('or', 218627),\n",
       " ('like', 202988),\n",
       " ('just', 200785),\n",
       " ('if', 194194),\n",
       " ('up', 193151),\n",
       " ('out', 187099),\n",
       " ('can', 183018),\n",
       " ('great', 179985),\n",
       " ('at', 177328),\n",
       " ('these', 176098),\n",
       " ('when', 175176),\n",
       " ('all', 171446),\n",
       " ('had', 166292),\n",
       " ('them', 164924),\n",
       " ('well', 161751),\n",
       " ('would', 161676),\n",
       " ('will', 158631),\n",
       " ('more', 147408),\n",
       " ('good', 145571),\n",
       " ('from', 144893),\n",
       " ('than', 139689)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_wo_punct.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Stop Words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text=[word for word in text if word.lower() not in stop_words]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in proccessed_data:\n",
    "    data[0] = remove_stopwords(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency of words after removing punctuation & stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of words after removing stop words: <FreqDist with 481023 samples and 27368065 outcomes>\n"
     ]
    }
   ],
   "source": [
    "fdist_wo_stop_words = FreqDist()\n",
    "for line in proccessed_data:\n",
    "    for word in line[0]:\n",
    "        fdist_wo_stop_words[word.lower()] += 1\n",
    "print(\"Frequency of words after removing stop words:\", fdist_wo_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('one', 270744),\n",
       " ('use', 243819),\n",
       " ('like', 202988),\n",
       " ('great', 179985),\n",
       " ('well', 161751),\n",
       " ('would', 161676),\n",
       " ('good', 145571),\n",
       " ('get', 136486),\n",
       " ('easy', 129296),\n",
       " ('time', 118144),\n",
       " ('really', 114609),\n",
       " ('dont', 113747),\n",
       " ('much', 109164),\n",
       " ('little', 106839),\n",
       " ('used', 106190),\n",
       " ('also', 106111),\n",
       " ('coffee', 105085),\n",
       " ('love', 103695),\n",
       " ('water', 100722),\n",
       " ('make', 94297),\n",
       " ('bought', 91486),\n",
       " ('nice', 89382),\n",
       " ('clean', 84660),\n",
       " ('product', 84548),\n",
       " ('made', 83895),\n",
       " ('even', 82685),\n",
       " ('works', 82030),\n",
       " ('im', 80594),\n",
       " ('using', 76578),\n",
       " ('put', 75824),\n",
       " ('ive', 75558),\n",
       " ('set', 75234),\n",
       " ('small', 74070),\n",
       " ('two', 73704),\n",
       " ('price', 71142),\n",
       " ('first', 68355),\n",
       " ('still', 66993),\n",
       " ('work', 65862),\n",
       " ('better', 65386),\n",
       " ('need', 65110),\n",
       " ('size', 63284),\n",
       " ('top', 62384),\n",
       " ('buy', 62174),\n",
       " ('quality', 61197),\n",
       " ('enough', 60855),\n",
       " ('pan', 60791),\n",
       " ('perfect', 60398),\n",
       " ('years', 59130),\n",
       " ('got', 58625),\n",
       " ('way', 56894)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common words in data set\n",
    "fdist_wo_stop_words.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WTF!! All of Stop words are suppossed to be filtered!\n",
    "stop_words_in_data = [word for word in fdist_wo_stop_words.keys() if word in stop_words]\n",
    "len(stop_words_in_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sentimental labels for helpfulness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask Gaston if 0.55 is a good threshold? Should I add Neutral label for 0 and %45 to %55??\n",
    "def create_sntmtl_label(help_list):\n",
    "    if help_list[1] == 0:\n",
    "        return -1\n",
    "#         return \"neutral\"\n",
    "    return 1 if (help_list[0] / help_list[1]) > 0.55 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in proccessed_data:\n",
    "    data[1] = create_sntmtl_label(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating sentimental labels for overall ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rating_sntmtl(rating: float):\n",
    "    if rating < 3:\n",
    "        return -1\n",
    "    elif rating == 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in proccessed_data:\n",
    "    line[2] = create_rating_sntmtl(line[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Panda dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(proccessed_data, columns=['Tokenized Review','Helpfulness','Rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized Review</th>\n",
       "      <th>Helpfulness</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[daughter, wanted, book, price, Amazon, best, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[bought, zoku, quick, pop, daughterr, zoku, qu...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[shortage, pop, recipes, available, free, web,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[book, must, get, Zoku, also, highly, recommen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[cookbook, great, really, enjoyed, reviewing, ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Tokenized Review  Helpfulness  Rating\n",
       "0  [daughter, wanted, book, price, Amazon, best, ...           -1       1\n",
       "1  [bought, zoku, quick, pop, daughterr, zoku, qu...           -1       1\n",
       "2  [shortage, pop, recipes, available, free, web,...            1       1\n",
       "3  [book, must, get, Zoku, also, highly, recommen...            1       1\n",
       "4  [cookbook, great, really, enjoyed, reviewing, ...           -1       1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Tokenized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokenized Review</th>\n",
       "      <th>Helpfulness</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['daughter', 'wanted', 'book', 'price', 'Amazo...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['bought', 'zoku', 'quick', 'pop', 'daughterr'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['shortage', 'pop', 'recipes', 'available', 'f...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['book', 'must', 'get', 'Zoku', 'also', 'highl...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cookbook', 'great', 'really', 'enjoyed', 're...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Tokenized Review  Helpfulness  Rating\n",
       "0  ['daughter', 'wanted', 'book', 'price', 'Amazo...           -1       1\n",
       "1  ['bought', 'zoku', 'quick', 'pop', 'daughterr'...           -1       1\n",
       "2  ['shortage', 'pop', 'recipes', 'available', 'f...            1       1\n",
       "3  ['book', 'must', 'get', 'Zoku', 'also', 'highl...            1       1\n",
       "4  ['cookbook', 'great', 'really', 'enjoyed', 're...           -1       1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"Tokenized.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(551682, 4)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Text\"]= df[\"Tokenized Review\"].map(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#split is 60 20 20\n",
    "X_train, X_test, __, _ = train_test_split(df[['Text',\"Helpfulness\",\"Rating\"]], df[['Rating']], test_size=0.20, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, __, _ = train_test_split(X_train, __, test_size=0.25, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"./data/Train.csv\",index=False)\n",
    "X_val.to_csv(\"./data/Val.csv\",index=False)\n",
    "X_test.to_csv(\"./data/Test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
