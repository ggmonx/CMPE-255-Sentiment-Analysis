{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642662b9",
   "metadata": {},
   "source": [
    "# Code adapted from\n",
    "* https://dzlab.github.io/dltips/en/pytorch/torchtext-datasets/ to use custom dataset\n",
    "* https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/3%20-%20Faster%20Sentiment%20Analysis.ipynb to use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b8aabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(' '.join(n_gram))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c290b7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy',\n",
    "                  tokenizer_language = 'en_core_web_sm',\n",
    "                  preprocessing = generate_bigrams)\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a1d58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128#64\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train, val, test = data.TabularDataset.splits(\n",
    "    path='./data', train='Train.csv',\n",
    "    validation='Val.csv', test='Test.csv', format='csv',\n",
    "    fields=[('text', TEXT),(None, None), ('label', LABEL)],\n",
    "    skip_header = True)# ignore helpfulness column\n",
    "    # columns are Text, helpfulness, and rating\n",
    "\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, val, test), batch_sizes=(BATCH_SIZE,BATCH_SIZE,BATCH_SIZE),\n",
    "    sort_key=lambda x: len(x.text), device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf810c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 50_000\n",
    "\n",
    "TEXT.build_vocab(train, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e310f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1) \n",
    "        \n",
    "        #pooled = [batch size, embedding_dim]\n",
    "                \n",
    "        return self.fc(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ba5a606",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "#model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b003cf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2768fe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6feedee91ec2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpretrained_embeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_embeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdadde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0627a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a147424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7703a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b70f193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea80e259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a70e3967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_model(adam=0,lr=0.001,epochs=5):\n",
    "    # use base adam if adam <=0.5 otherwise use adamW with weight decay\n",
    "    model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr) if adam<=0.5 else optim.AdamW(model.parameters(),lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    N_EPOCHS = round(epochs)\n",
    "    #best_valid_loss = float('inf')\n",
    "    start_time = time.time()\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        \n",
    "\n",
    "        train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
    "\n",
    "       \n",
    "\n",
    "        \n",
    "        \n",
    "       # if valid_loss < best_valid_loss:\n",
    "       #     best_valid_loss = valid_loss\n",
    "       #     torch.save(model.state_dict(), 'sentiment.pt')\n",
    "        \n",
    "        print(f'Epoch: {epoch+1:02}')# | Epoch Time: {epoch_mins}m {epoch_secs}s'\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    print(f'Total Train time for {N_EPOCHS} epochs: {epoch_mins}m {epoch_secs}s')\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    return -valid_loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ccd089f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |   adam    |  epochs   |    lr     |\n",
      "-------------------------------------------------------------\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.308 | Train Acc: 81.33%\n",
      "\t Val. Loss: 1.279 |  Val. Acc: 76.31%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.210 | Train Acc: 80.72%\n",
      "\t Val. Loss: 1.743 |  Val. Acc: 74.17%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.167 | Train Acc: 80.24%\n",
      "\t Val. Loss: 1.880 |  Val. Acc: 77.66%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.133 | Train Acc: 79.96%\n",
      "\t Val. Loss: 2.815 |  Val. Acc: 74.64%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.100 | Train Acc: 79.61%\n",
      "\t Val. Loss: 2.885 |  Val. Acc: 76.97%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.066 | Train Acc: 79.48%\n",
      "\t Val. Loss: 4.562 |  Val. Acc: 78.17%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.025 | Train Acc: 79.36%\n",
      "\t Val. Loss: 6.714 |  Val. Acc: 79.72%\n",
      "Epoch: 08\n",
      "\tTrain Loss: -0.032 | Train Acc: 79.29%\n",
      "\t Val. Loss: 10.629 |  Val. Acc: 75.81%\n",
      "Epoch: 09\n",
      "\tTrain Loss: -0.113 | Train Acc: 79.21%\n",
      "\t Val. Loss: 14.944 |  Val. Acc: 77.78%\n",
      "Total Train time for 9 epochs: 16m 2s\n",
      "------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-14.94   \u001b[0m | \u001b[0m 0.1915  \u001b[0m | \u001b[0m 8.599   \u001b[0m | \u001b[0m 0.02189 \u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.314 | Train Acc: 81.65%\n",
      "\t Val. Loss: 1.091 |  Val. Acc: 77.11%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.220 | Train Acc: 81.01%\n",
      "\t Val. Loss: 1.311 |  Val. Acc: 79.57%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.197 | Train Acc: 80.76%\n",
      "\t Val. Loss: 1.449 |  Val. Acc: 80.87%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.186 | Train Acc: 80.57%\n",
      "\t Val. Loss: 1.388 |  Val. Acc: 78.94%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.181 | Train Acc: 80.57%\n",
      "\t Val. Loss: 1.466 |  Val. Acc: 76.49%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.180 | Train Acc: 80.51%\n",
      "\t Val. Loss: 2.089 |  Val. Acc: 71.56%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.182 | Train Acc: 80.45%\n",
      "\t Val. Loss: 1.591 |  Val. Acc: 77.75%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.180 | Train Acc: 80.47%\n",
      "\t Val. Loss: 1.561 |  Val. Acc: 80.73%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.177 | Train Acc: 80.48%\n",
      "\t Val. Loss: 1.556 |  Val. Acc: 79.06%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.177 | Train Acc: 80.48%\n",
      "\t Val. Loss: 1.597 |  Val. Acc: 77.70%\n",
      "Total Train time for 10 epochs: 18m 2s\n",
      "------------------------------------------------------------\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-1.597   \u001b[0m | \u001b[95m 0.7854  \u001b[0m | \u001b[95m 10.02   \u001b[0m | \u001b[95m 0.01364 \u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.309 | Train Acc: 81.25%\n",
      "\t Val. Loss: 1.046 |  Val. Acc: 77.89%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.216 | Train Acc: 80.60%\n",
      "\t Val. Loss: 1.753 |  Val. Acc: 79.68%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.173 | Train Acc: 80.19%\n",
      "\t Val. Loss: 1.520 |  Val. Acc: 77.87%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.145 | Train Acc: 79.96%\n",
      "\t Val. Loss: 1.841 |  Val. Acc: 77.34%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.114 | Train Acc: 79.65%\n",
      "\t Val. Loss: 2.029 |  Val. Acc: 78.21%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.080 | Train Acc: 79.41%\n",
      "\t Val. Loss: 4.160 |  Val. Acc: 78.49%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.050 | Train Acc: 79.34%\n",
      "\t Val. Loss: 5.309 |  Val. Acc: 77.96%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.004 | Train Acc: 79.19%\n",
      "\t Val. Loss: 9.446 |  Val. Acc: 75.18%\n",
      "Epoch: 09\n",
      "\tTrain Loss: -0.081 | Train Acc: 79.19%\n",
      "\t Val. Loss: 16.046 |  Val. Acc: 77.05%\n",
      "Epoch: 10\n",
      "\tTrain Loss: -0.245 | Train Acc: 79.16%\n",
      "\t Val. Loss: 28.163 |  Val. Acc: 75.89%\n",
      "Total Train time for 10 epochs: 18m 34s\n",
      "------------------------------------------------------------\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-28.16   \u001b[0m | \u001b[0m 0.2765  \u001b[0m | \u001b[0m 10.22   \u001b[0m | \u001b[0m 0.04791 \u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.312 | Train Acc: 81.38%\n",
      "\t Val. Loss: 1.130 |  Val. Acc: 76.16%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.240 | Train Acc: 80.93%\n",
      "\t Val. Loss: 1.347 |  Val. Acc: 75.55%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.228 | Train Acc: 80.76%\n",
      "\t Val. Loss: 1.165 |  Val. Acc: 77.80%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.227 | Train Acc: 80.84%\n",
      "\t Val. Loss: 1.255 |  Val. Acc: 78.58%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.226 | Train Acc: 80.76%\n",
      "\t Val. Loss: 1.299 |  Val. Acc: 78.88%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.230 | Train Acc: 80.76%\n",
      "\t Val. Loss: 1.228 |  Val. Acc: 79.38%\n",
      "Total Train time for 6 epochs: 11m 5s\n",
      "------------------------------------------------------------\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-1.228   \u001b[0m | \u001b[95m 0.8759  \u001b[0m | \u001b[95m 6.22    \u001b[0m | \u001b[95m 0.02505 \u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.311 | Train Acc: 81.49%\n",
      "\t Val. Loss: 1.183 |  Val. Acc: 75.87%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.229 | Train Acc: 80.91%\n",
      "\t Val. Loss: 1.224 |  Val. Acc: 77.58%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.211 | Train Acc: 80.79%\n",
      "\t Val. Loss: 1.304 |  Val. Acc: 79.68%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.206 | Train Acc: 80.69%\n",
      "\t Val. Loss: 1.366 |  Val. Acc: 79.45%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.203 | Train Acc: 80.64%\n",
      "\t Val. Loss: 1.387 |  Val. Acc: 76.60%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.202 | Train Acc: 80.58%\n",
      "\t Val. Loss: 1.454 |  Val. Acc: 75.68%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.203 | Train Acc: 80.61%\n",
      "\t Val. Loss: 1.367 |  Val. Acc: 79.45%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.204 | Train Acc: 80.62%\n",
      "\t Val. Loss: 1.289 |  Val. Acc: 79.01%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.205 | Train Acc: 80.58%\n",
      "\t Val. Loss: 1.425 |  Val. Acc: 75.45%\n",
      "Total Train time for 9 epochs: 16m 38s\n",
      "------------------------------------------------------------\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-1.425   \u001b[0m | \u001b[0m 0.6835  \u001b[0m | \u001b[0m 9.414   \u001b[0m | \u001b[0m 0.01852 \u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.318 | Train Acc: 81.23%\n",
      "\t Val. Loss: 1.093 |  Val. Acc: 78.91%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.268 | Train Acc: 80.94%\n",
      "\t Val. Loss: 1.305 |  Val. Acc: 77.42%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.263 | Train Acc: 80.88%\n",
      "\t Val. Loss: 1.193 |  Val. Acc: 80.79%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.264 | Train Acc: 80.93%\n",
      "\t Val. Loss: 1.241 |  Val. Acc: 76.45%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.267 | Train Acc: 80.91%\n",
      "\t Val. Loss: 1.070 |  Val. Acc: 75.12%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.263 | Train Acc: 80.93%\n",
      "\t Val. Loss: 1.311 |  Val. Acc: 77.28%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.264 | Train Acc: 80.86%\n",
      "\t Val. Loss: 1.124 |  Val. Acc: 75.17%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.264 | Train Acc: 80.97%\n",
      "\t Val. Loss: 1.312 |  Val. Acc: 81.34%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.265 | Train Acc: 80.87%\n",
      "\t Val. Loss: 1.140 |  Val. Acc: 75.00%\n",
      "Total Train time for 9 epochs: 16m 16s\n",
      "------------------------------------------------------------\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m-1.14    \u001b[0m | \u001b[95m 0.7809  \u001b[0m | \u001b[95m 9.317   \u001b[0m | \u001b[95m 0.04597 \u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.695 | Train Acc: 32.73%\n",
      "\t Val. Loss: 0.585 |  Val. Acc: 82.58%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.676 | Train Acc: 72.65%\n",
      "\t Val. Loss: 0.569 |  Val. Acc: 82.60%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.663 | Train Acc: 82.25%\n",
      "\t Val. Loss: 0.603 |  Val. Acc: 82.60%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.653 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.653 |  Val. Acc: 82.60%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.647 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.695 |  Val. Acc: 82.60%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.642 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.724 |  Val. Acc: 82.60%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.639 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.738 |  Val. Acc: 82.60%\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.636 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.744 |  Val. Acc: 82.60%\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.632 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.744 |  Val. Acc: 82.60%\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.628 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.740 |  Val. Acc: 82.60%\n",
      "Total Train time for 10 epochs: 18m 6s\n",
      "------------------------------------------------------------\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m-0.7399  \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 9.685   \u001b[0m | \u001b[95m 1e-05   \u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.314 | Train Acc: 81.34%\n",
      "\t Val. Loss: 1.304 |  Val. Acc: 78.55%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.250 | Train Acc: 80.99%\n",
      "\t Val. Loss: 1.341 |  Val. Acc: 75.88%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.243 | Train Acc: 80.85%\n",
      "\t Val. Loss: 1.134 |  Val. Acc: 75.58%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.239 | Train Acc: 80.87%\n",
      "\t Val. Loss: 1.133 |  Val. Acc: 76.34%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.238 | Train Acc: 80.81%\n",
      "\t Val. Loss: 1.256 |  Val. Acc: 78.78%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.242 | Train Acc: 80.82%\n",
      "\t Val. Loss: 1.239 |  Val. Acc: 79.66%\n",
      "Total Train time for 6 epochs: 10m 54s\n",
      "------------------------------------------------------------\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-1.239   \u001b[0m | \u001b[0m 0.8275  \u001b[0m | \u001b[0m 5.594   \u001b[0m | \u001b[0m 0.03092 \u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.636 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.692 |  Val. Acc: 82.60%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.594 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.580 |  Val. Acc: 82.60%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.559 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.560 |  Val. Acc: 83.00%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.527 | Train Acc: 82.46%\n",
      "\t Val. Loss: 0.565 |  Val. Acc: 83.41%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.498 | Train Acc: 82.39%\n",
      "\t Val. Loss: 0.589 |  Val. Acc: 82.82%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06\n",
      "\tTrain Loss: 0.471 | Train Acc: 82.52%\n",
      "\t Val. Loss: 0.611 |  Val. Acc: 82.44%\n",
      "Total Train time for 6 epochs: 10m 49s\n",
      "------------------------------------------------------------\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m-0.6107  \u001b[0m | \u001b[95m 0.2745  \u001b[0m | \u001b[95m 5.952   \u001b[0m | \u001b[95m 0.000133\u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.309 | Train Acc: 81.19%\n",
      "\t Val. Loss: 1.035 |  Val. Acc: 78.49%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.217 | Train Acc: 80.59%\n",
      "\t Val. Loss: 1.321 |  Val. Acc: 78.39%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.173 | Train Acc: 80.16%\n",
      "\t Val. Loss: 1.562 |  Val. Acc: 75.92%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.141 | Train Acc: 79.93%\n",
      "\t Val. Loss: 2.331 |  Val. Acc: 76.76%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.110 | Train Acc: 79.62%\n",
      "\t Val. Loss: 2.571 |  Val. Acc: 79.10%\n",
      "Total Train time for 5 epochs: 9m 9s\n",
      "------------------------------------------------------------\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-2.571   \u001b[0m | \u001b[0m 0.1746  \u001b[0m | \u001b[0m 5.277   \u001b[0m | \u001b[0m 0.05    \u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.309 | Train Acc: 81.19%\n",
      "\t Val. Loss: 1.183 |  Val. Acc: 74.00%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.218 | Train Acc: 80.62%\n",
      "\t Val. Loss: 1.415 |  Val. Acc: 78.14%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.175 | Train Acc: 80.23%\n",
      "\t Val. Loss: 2.506 |  Val. Acc: 74.97%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.145 | Train Acc: 79.95%\n",
      "\t Val. Loss: 2.029 |  Val. Acc: 76.29%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.115 | Train Acc: 79.64%\n",
      "\t Val. Loss: 2.856 |  Val. Acc: 76.94%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.086 | Train Acc: 79.48%\n",
      "\t Val. Loss: 3.860 |  Val. Acc: 75.62%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.053 | Train Acc: 79.22%\n",
      "\t Val. Loss: 4.380 |  Val. Acc: 78.69%\n",
      "Total Train time for 7 epochs: 12m 31s\n",
      "------------------------------------------------------------\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-4.38    \u001b[0m | \u001b[0m 0.2895  \u001b[0m | \u001b[0m 6.641   \u001b[0m | \u001b[0m 0.05    \u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.316 | Train Acc: 81.23%\n",
      "\t Val. Loss: 1.059 |  Val. Acc: 75.73%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.264 | Train Acc: 80.98%\n",
      "\t Val. Loss: 1.195 |  Val. Acc: 80.39%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.255 | Train Acc: 80.95%\n",
      "\t Val. Loss: 1.152 |  Val. Acc: 77.30%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.254 | Train Acc: 80.82%\n",
      "\t Val. Loss: 1.142 |  Val. Acc: 78.46%\n",
      "Total Train time for 4 epochs: 7m 2s\n",
      "------------------------------------------------------------\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-1.142   \u001b[0m | \u001b[0m 0.7424  \u001b[0m | \u001b[0m 4.485   \u001b[0m | \u001b[0m 0.03939 \u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.315 | Train Acc: 81.37%\n",
      "\t Val. Loss: 1.140 |  Val. Acc: 80.74%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.245 | Train Acc: 81.00%\n",
      "\t Val. Loss: 1.125 |  Val. Acc: 78.33%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.236 | Train Acc: 80.82%\n",
      "\t Val. Loss: 1.265 |  Val. Acc: 74.99%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.236 | Train Acc: 80.76%\n",
      "\t Val. Loss: 1.400 |  Val. Acc: 73.76%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.234 | Train Acc: 80.83%\n",
      "\t Val. Loss: 1.307 |  Val. Acc: 76.01%\n",
      "Total Train time for 5 epochs: 8m 53s\n",
      "------------------------------------------------------------\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-1.307   \u001b[0m | \u001b[0m 0.988   \u001b[0m | \u001b[0m 4.986   \u001b[0m | \u001b[0m 0.02836 \u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.686 | Train Acc: 65.66%\n",
      "\t Val. Loss: 0.594 |  Val. Acc: 82.60%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.668 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.568 |  Val. Acc: 82.60%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.655 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.590 |  Val. Acc: 82.60%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.647 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.633 |  Val. Acc: 82.60%\n",
      "Total Train time for 4 epochs: 7m 6s\n",
      "------------------------------------------------------------\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m-0.6329  \u001b[0m | \u001b[0m 0.03604 \u001b[0m | \u001b[0m 4.426   \u001b[0m | \u001b[0m 1e-05   \u001b[0m |\n",
      "Epoch: 01\n",
      "\tTrain Loss: 0.306 | Train Acc: 81.23%\n",
      "\t Val. Loss: 1.067 |  Val. Acc: 79.86%\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.216 | Train Acc: 80.72%\n",
      "\t Val. Loss: 1.440 |  Val. Acc: 77.27%\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.170 | Train Acc: 80.23%\n",
      "\t Val. Loss: 1.849 |  Val. Acc: 79.16%\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.134 | Train Acc: 79.91%\n",
      "\t Val. Loss: 2.367 |  Val. Acc: 78.92%\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.106 | Train Acc: 79.59%\n",
      "\t Val. Loss: 2.464 |  Val. Acc: 78.82%\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.068 | Train Acc: 79.46%\n",
      "\t Val. Loss: 4.439 |  Val. Acc: 76.84%\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.037 | Train Acc: 79.28%\n",
      "\t Val. Loss: 6.349 |  Val. Acc: 76.59%\n",
      "Epoch: 08\n",
      "\tTrain Loss: -0.020 | Train Acc: 79.21%\n",
      "\t Val. Loss: 10.774 |  Val. Acc: 77.10%\n",
      "Total Train time for 8 epochs: 14m 8s\n",
      "------------------------------------------------------------\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-10.77   \u001b[0m | \u001b[0m 0.4317  \u001b[0m | \u001b[0m 8.21    \u001b[0m | \u001b[0m 0.03254 \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "# Bounded region of parameter space\n",
    "pbounds = {'adam':(0,1),'lr': (0.00001, 0.05), 'epochs': (3, 12)}\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_model,\n",
    "    pbounds=pbounds,\n",
    "    random_state=SEED,\n",
    ")\n",
    "optimizer.maximize(\n",
    "    init_points=5,\n",
    "    n_iter=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3313e1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'target': -0.610654134467365, 'params': {'adam': 0.2745473095826844, 'epochs': 5.952006228437811, 'lr': 0.00013297883561406217}}\n",
      "Iteration 0: \n",
      "\t{'target': -14.94432977598208, 'params': {'adam': 0.1915194503788923, 'epochs': 8.598978939358487, 'lr': 0.021892009672965652}}\n",
      "Iteration 1: \n",
      "\t{'target': -1.597478325667587, 'params': {'adam': 0.7853585837137692, 'epochs': 10.019782273069232, 'lr': 0.013636904338079254}}\n",
      "Iteration 2: \n",
      "\t{'target': -28.163462447401823, 'params': {'adam': 0.2764642551430967, 'epochs': 10.216849597815173, 'lr': 0.047907386290648425}}\n",
      "Iteration 3: \n",
      "\t{'target': -1.2275178161939944, 'params': {'adam': 0.8759326347420947, 'epochs': 6.2203554296208, 'lr': 0.0250547463249177}}\n",
      "Iteration 4: \n",
      "\t{'target': -1.4251740560591704, 'params': {'adam': 0.6834629351721363, 'epochs': 9.414318242846102, 'lr': 0.018518835231971842}}\n",
      "Iteration 5: \n",
      "\t{'target': -1.1399453700922628, 'params': {'adam': 0.7809393108395023, 'epochs': 9.31704812255748, 'lr': 0.04597356133128867}}\n",
      "Iteration 6: \n",
      "\t{'target': -0.7399134932955682, 'params': {'adam': 1.0, 'epochs': 9.685310906502776, 'lr': 1e-05}}\n",
      "Iteration 7: \n",
      "\t{'target': -1.238811945922805, 'params': {'adam': 0.8275215928122288, 'epochs': 5.593888635209621, 'lr': 0.03092129729138969}}\n",
      "Iteration 8: \n",
      "\t{'target': -0.610654134467365, 'params': {'adam': 0.2745473095826844, 'epochs': 5.952006228437811, 'lr': 0.00013297883561406217}}\n",
      "Iteration 9: \n",
      "\t{'target': -2.5711432310686653, 'params': {'adam': 0.17455092964616925, 'epochs': 5.277353692988167, 'lr': 0.05}}\n",
      "Iteration 10: \n",
      "\t{'target': -4.3803758097689505, 'params': {'adam': 0.2894879733593897, 'epochs': 6.640836196754041, 'lr': 0.05}}\n",
      "Iteration 11: \n",
      "\t{'target': -1.1423733783160095, 'params': {'adam': 0.7423548737973155, 'epochs': 4.484955438355787, 'lr': 0.03939002186521719}}\n",
      "Iteration 12: \n",
      "\t{'target': -1.3070105891480506, 'params': {'adam': 0.9879924898822857, 'epochs': 4.98611828126392, 'lr': 0.028356115680072014}}\n",
      "Iteration 13: \n",
      "\t{'target': -0.6329170589012592, 'params': {'adam': 0.03604280941062306, 'epochs': 4.425919374329952, 'lr': 1e-05}}\n",
      "Iteration 14: \n",
      "\t{'target': -10.774113656817002, 'params': {'adam': 0.4317002448107953, 'epochs': 8.210313735081982, 'lr': 0.03254010502542471}}\n"
     ]
    }
   ],
   "source": [
    "print(optimizer.max)\n",
    "for i, res in enumerate(optimizer.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ace4f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_save(adam=0,lr=0.001,epochs=5):\n",
    "    # use base adam if adam <=0.5 otherwise use adamW with weight decay\n",
    "    model = FastText(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)\n",
    "    pretrained_embeddings = TEXT.vocab.vectors\n",
    "    model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "    UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    optimizer = optim.Adam(model.parameters(),lr=lr) if adam<=0.5 else optim.AdamW(model.parameters(),lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    N_EPOCHS = round(epochs)\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "\n",
    "    for epoch in range(N_EPOCHS):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, val_iter, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'sentiment.pt')\n",
    "\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45d8715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 40s\n",
      "\tTrain Loss: 0.629 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 82.60%\n",
      "Epoch: 02 | Epoch Time: 1m 42s\n",
      "\tTrain Loss: 0.589 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.574 |  Val. Acc: 82.60%\n",
      "Epoch: 03 | Epoch Time: 1m 41s\n",
      "\tTrain Loss: 0.554 | Train Acc: 82.47%\n",
      "\t Val. Loss: 0.551 |  Val. Acc: 83.31%\n",
      "Epoch: 04 | Epoch Time: 1m 41s\n",
      "\tTrain Loss: 0.524 | Train Acc: 82.42%\n",
      "\t Val. Loss: 0.577 |  Val. Acc: 83.47%\n",
      "Epoch: 05 | Epoch Time: 1m 40s\n",
      "\tTrain Loss: 0.496 | Train Acc: 82.41%\n",
      "\t Val. Loss: 0.588 |  Val. Acc: 82.81%\n",
      "Epoch: 06 | Epoch Time: 1m 40s\n",
      "\tTrain Loss: 0.469 | Train Acc: 82.57%\n",
      "\t Val. Loss: 0.598 |  Val. Acc: 82.15%\n"
     ]
    }
   ],
   "source": [
    "model = train_model_save(adam= optimizer.max['params']['adam'], lr=optimizer.max['params']['lr'],\n",
    "                 epochs=optimizer.max['params']['epochs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93f903e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.554 | Test Acc: 83.32%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('sentiment.pt'))\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "test_loss, test_acc = evaluate(model, test_iter, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e66b3f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = generate_bigrams([tok.text for tok in nlp.tokenizer(sentence)])\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47fde58d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9986447691917419"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"this book was bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78b006ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9877820611000061"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"this movie is good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c4284",
   "metadata": {},
   "source": [
    "# Problems\n",
    "- Some words are not processed correctly: good has high sentiment but so does bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf4f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv(\"Tokenized.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec727370",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Prediction\"]= df[\"Text\"].map(lambda x:predict_sentiment(model, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94faf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Scaled_Prediction\"]= df.apply(lambda row:(row['Prediction']*row['Helpfulness']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb238c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af0f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"results.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0375e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212352bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
